# -*- coding: utf-8 -*-
"""My_Neural_Network_Class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CcMclkPchtbS1kPmRyvu9m32O4pMpZ4f
"""

import numpy as np

RELU = "relu"
SIGMOID = "sigmoid"

"""
Relu Function:
Deriv is True
If x > 0 then return 1
Else return 0

Deriv is False
If x > 0 then return x
Else return 0
"""
def relu(x, deriv=False):
    if deriv:
      return x > 0
    return (x > 0) * x

"""
Sigmoid Function:
"""
def sigmoid(x, deriv=False):
  if deriv:
    return x*(1-x)
  return 1 / (1 + np.exp(-x))

class NeuralNetwork:
  def __init__(self):
    np.random.seed(1)  # Same input for weights and biases for all tests
    self.layers = 3 # Number of layers
    self.layers_size = np.array([3, 4, 1])  # Number of tensors for each layer
    self.epochs = 10  # Number of iterations for training
    self.activation_function = RELU  # Activation function to be used
    self.alpha = 0.1  # Learning rate for weights
    self.alpha_bias = 0.1  # Learning rate for biases
  
  # User can change the default parameters for a variety of neural network testing
  def set_network(self, layers: int, layers_size: np.array, epochs: int, activation_function = RELU, alpha=0.1, alpha_bias=0.1):
    self.layers = layers  
    self.layers_size = layers_size
    self.epochs = epochs
    self.activation_function = activation_function
    self.alpha = alpha
    self.alpha_bias = alpha_bias
  
  # Returns the output of the selected activation function
  def Af(self, x, deriv = False):
    if self.activation_function == SIGMOID:
      return sigmoid(x, deriv)
    return relu(x, deriv=deriv)
  
  # Train the network
  def train(self, input, output, info_print=False):
    self.weights = []  # Create a weight matrix
    self.bias = []  # Create a bias matrix  
    for i in range(self.layers-1):  # For each layer before last
      self.weights.append(2*np.random.random((self.layers_size[i], self.layers_size[i+1])) - 1)  # Create a weight matrix with shape (L_curr_tensors, L_next_tensors)
      self.bias.append(2*np.random.random((self.layers_size[i+1], 1)) - 1) # Create a bias matrix with shape (L_next_tensors)
    # Transform Lists to np.array()
    self.weights = np.array(self.weights)
    self.bias = np.array(self.bias)

    data_size = len(input)  # Store the size of the data to a variable
    loss_error = 0  # Create a loss_error variable
    for iteration in range(self.epochs):  # For each epoch
      final_error = 0  # Set final_error to 0 (epoch error)
      for i in range(data_size):  # For every data
        layer_data = []  # Create a layer matrix (store the value of tensors)
        for layer in range(self.layers):  # For each layer
          # Forward Process
          if layer == 0:  # If layer is 0
            layer_data.append(np.array(input[i:i+1])) # Input data to the first layer
          elif layer > 0 and layer < self.layers - 1:  # Else if hidden layer
            layer_data.append(np.dot(layer_data[layer-1], self.weights[layer-1]) + self.bias[layer-1].T)  # Append hidden tensor values
          elif layer == self.layers - 1:  # Else if output layer
            layer_data.append(self.Af(np.dot(layer_data[layer-1], self.weights[layer-1]) + self.bias[layer-1].T, deriv=False))  # Append final result

            # Backpropagation
            # Calculate Errors
            b_index = self.layers - 1 # Create an index to track the previous layer
            delta_error = []  # Create a matrix to store to erros from output to input
            for b_layer in range(self.layers - 1):  # For each layer (backward process)
              if b_layer == 0:  # If b_layer is 0 (we are at output layer)
                delta_error.append(layer_data[b_index] - output[i:i+1])  # delta_error = predicted_output - expected_output
                final_error += delta_error[b_layer] ** 2  # Store the squared error
              elif b_layer > 0 and b_layer < self.layers - 1: # Else for each hidden layer
                delta_error_prev = delta_error[b_layer-1] # Store previous delta error
                weights_current = self.weights[b_index] # Store the transposed weighted matrix
                delta_error.append(np.dot(weights_current, delta_error_prev) * self.Af(layer_data[b_index], deriv=True))
                # delta_error = dot(delta_error_prev, weights_current) + self.bias(current)
              b_index -= 1  # Go back a layer
            # Calculate New Weights
            b_index = self.layers - 1  # Create a variable for backpropagation indexes
            w_index = self.layers - 2  # Create a variable for weight indexing
            for b_layer in range(self.layers - 1):
              if b_layer >= 0 and b_layer < self.layers - 2:
                layer_prev = np.array(layer_data[b_index-1]).T
                layer_delta_curr = delta_error[b_layer]
                self.weights[w_index] -= self.alpha * np.dot(layer_prev, layer_delta_curr)
                self.bias[w_index] -= self.alpha_bias * layer_delta_curr
                b_index -= 1
                w_index -= 1
      if info_print and iteration % 10 == 9:
        print("\n")
        print("Error: %.2e" % final_error)
        for i in range(self.layers-1):
          print("Weights_%d:" % i + str(self.weights[i]))
      loss_error = final_error

    loss_size = len(loss_error.T)
    print("\n")
    for i in range(loss_size):
      print("LOSS ERROR_%d" % i + " = %0.2e" % loss_error[i])
      
  def test(self, input):
    output = input
    for i in range(self.layers-1):
      output = np.dot(output, self.weights[i]) + self.bias[i].T
    return self.Af(output, deriv=False)

if __name__ == "__main__":
  streetlight = np.array( [ [1, 0, 1],
                            [0, 1, 1],
                            [0, 0, 1],
                            [1, 1, 0],
                            [1, 0, 0]] )

  # walk_vs_stop = np.array([[1,0], [0,1], [0,0], [1,1], [1,0])
  walk_vs_stop = np.array([[1], [0], [0], [1], [0]])

  nn = NeuralNetwork()
  nn.set_network(layers=3, layers_size=np.array([3,4,1]), epochs=100000, activation_function=SIGMOID, alpha=0.3, alpha_bias=0.3)
  nn.train(streetlight, walk_vs_stop, info_print=False)

loop = True
  while loop:
    print("\n")
    A = input("A (0 or 1) =  ")
    B = input("B (0 or 1) =  ")
    C = input("C (0 or 1) =  ")
    if (A == "0" or A == "1") and (B == "0" or B == "1") and (C == "0" or C == "1"):
      test_input = np.array([[int(A), int(B), int(C)]])
      output = nn.test(test_input)

      output_size = len(output.T)
      print("\n")
      for i in range(output_size):
        print("Output_%d" % i + " = %0.2f" % output.T[i])
    else:
      loop = False